---
title: Regression Diagnostics in R
# author: Kyle Thomas
# category: tech
# tags: r, regression
# summary: Some simple regression diagnostics
pdf_document: default
output:
  html_notebook:
    pandoc_args:
    - --number-sections
    toc: yes
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

# Overview 

Regression in general looking at variations

# Data Description

will be using the very sexy abalone dataset from UCI Machine Learning repository

```{r data_prep}
# load libraries
library(ggplot2) #ggplot for better looking graphics
library(ISLR) #datasets from ISLR
library(car) #regression tools
library(AppliedPredictiveModeling) #UCI datasets
library(corrplot) #for correlations plot

# load the abalone data
data("abalone")
abaDat <- abalone
# apply column names
colnames(abaDat) <-c("sex","len","diam","h","ww","sw","vw","sh","rings")
# add 1.5 to get the predict variable
abaDat$age <- abaDat$rings+1.5

# look at the dimensions of the data
dim(abaDat)
```


note on package of appliedpredict... because didn't want to link to my own dataset

# Correlation

formula for correlation; diffs in spearman vs pearson

```{r corr}

# get spearman correlations
scor <- cor(abaDat[,2:ncol(abaDat)],method="spearman")
scor

# visualize
corrplot.mixed(scor)

# get pearson correlations
pcor <- cor(abaDat[,2:ncol(abaDat)],method="pearson")
pcor

# visualize
corrplot.mixed(pcor)

```

# Regression

```{r linear_model}

# regress age on shell weight
fit <- lm(age~sh,abaDat)

# summary of results1
summary(fit)

r2<-round(summary(fit)$r.squared,2)
rse<-round(summary(fit)$sigma,2)

```

The $R^2$ tells us that shell weight explains about 40% of the variation in the age variable. Given that the coefficients in the model have p-values approximating zero, we can say that this is a statistically significant predictor.

The $R^2$ is `r r2` and the RSE is `r rse`.

$\hat{\beta}_1$ tells us that for every unit increase in shell weight, age increases by 14 units. The intercept is the average value of age if shell weight is equal to zero. However, neither age nor shell can equal zero so the intercept is meaningless

```{r scatter}

# scatterplot with fitted line
plot(abaDat$sh,abaDat$age,xlab="Shell Weight",ylab="Age")
abline(fit,col="red")

```

# Diagnostics

```{r diag_plots}

# diagnostic plots
par(mfrow=c(2,2))
plot(fit)

```

+ The fitted line plot and the residuals vs fitted values plot both suggest a nonlinearity in the relationship between the two variables. The QQ plot also shows us that the residuals violate the assumption of normally distributed errors. The Scale Location plot suggests there is heteroskadasticity while the leverage plot indicates there are a few levered points of relatively small magnitude.

# Intervals

```{r confint}

# confidence intervals
confint(fit)

```

```{r predict}

# new values
new.vals <- data.frame(sh=c(0.1,0.2,0.3))

# confidence intervals for new values
predict(fit, newdata=new.vals, interval="confidence")

# prediction intervals for new values
predict(fit, newdata=new.vals, interva="prediction")

```

+ The confidence interval for parameters and predictions give us a range of values that, if the experiment were repeated 100 times, 95 of those times the parameter/prediction would fall within that range.
+ Confidence intervals are centered around averages while prediction intervals are centered around a particular instance of a variable.
+ Prediction intervals are wider because their standard error is larger; it is larger because individual variations in the data are larger than the average variations of all variable observations.

# Transformation

```{r log_lm}

# log transformed fit
fit_log<-lm(log(age)~log(sh),abaDat)

# summary
summary(fit_log)

# get r2
r2_log <- round(summary(fit_log)$r.squared,2)

```

+ Not directly, you could get a general sense of magnitude and direction by appropriately interpreting the log-log relationship in linear terms.
+ On a very basic level, RSE is in the model's units and since the log-transformed model has different units than the linear model, the two cannot be compared.
+ $\textsection$ 3.1.3 of ISLR tells us that $R^2$ is unit less compared to RSE, which would lead us to believe that it could be compared across log transformed models. However, this would be incorrect because the variations being compared in a linear vs a log transformed variables are different.
+ We can interpret the coefficients to mean that a 1% increase in age is associated with a .24% increase in shell weight, on average
+ The intercept here is still meaningless because the age still cannot be zero.

```{r log_scatter}

# scatterplot with fitted line
plot(log(abaDat$sh),log(abaDat$age),xlab="log(Shell Weight)",ylab="log(Age)")
abline(fit_log,col="red")

```

There appears to be a better linear fit, however, some nonlinearity still appears as the log of shell weight increases.

```{r diag_plots_log}

# diagnostic plots
par(mfrow=c(2,2))
plot(fit_log)

```

The residuals seem to be more randomly distributed above and below the line, however, heteroskedasticity is still in place. The QQ plot also tells us that the residuals seem to be more closely distributed normally.



```{r log_multi}

# model
log_mfit<-lm(log(age)~log(sh)+log(sw),abaDat)

# summary
summary(log_mfit)
r2_mfit<-round(summary(log_mfit)$r.squared,2)

# confidence intervals
confint(log_mfit)

# diagnostic plots
par(mfrow=c(2,2))
plot(log_mfit)

```

Adding the log of shucked weight helps with the predictive power of the model. The $R^2$ increases from `r r2_log` to `r r2_mfit` and all there $\hat{\beta}$s are significant with p-values approaching zero. While there are a few leverage points, the residuals seem to be less heteroskedastic, have less patterns in their distributions over fitted values, and have a more normal distribution. The biggest downside of adding log of shucked weight is that interpreting OLS models with multiple predictors is not as straight forward as interpreting models with only one predictor.


